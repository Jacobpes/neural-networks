{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2168deef-faf1-4306-9b7c-3807aa277506",
   "metadata": {},
   "source": [
    "# Neural Networks AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7ab90-0ed5-49c2-b41c-3e985463923c",
   "metadata": {},
   "source": [
    "## Exercise 0: Environment and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1f1c45-5003-4653-af37-91a6c7204f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65567860-d4cd-4ecc-ba4e-a3e544f8aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ---------------\n",
      "anyio                     4.2.0\n",
      "appnope                   0.1.3\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "blinker                   1.7.0\n",
      "certifi                   2023.11.17\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "click                     8.1.7\n",
      "comm                      0.2.1\n",
      "contourpy                 1.2.0\n",
      "cycler                    0.12.1\n",
      "dash                      2.15.0\n",
      "dash-core-components      2.0.0\n",
      "dash-html-components      2.0.0\n",
      "dash-table                5.0.0\n",
      "debugpy                   1.8.0\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.1\n",
      "Flask                     3.0.1\n",
      "fonttools                 4.47.2\n",
      "fqdn                      1.5.1\n",
      "idna                      3.6\n",
      "importlib-metadata        7.0.1\n",
      "ipykernel                 6.29.0\n",
      "ipython                   8.21.0\n",
      "ipywidgets                8.1.1\n",
      "isoduration               20.11.0\n",
      "itsdangerous              2.1.2\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "joblib                    1.3.2\n",
      "json5                     0.9.14\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter                   1.0.0\n",
      "jupyter_client            8.6.0\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.1\n",
      "jupyter-events            0.9.0\n",
      "jupyter-lsp               2.2.2\n",
      "jupyter_server            2.12.5\n",
      "jupyter_server_terminals  0.5.2\n",
      "jupyterlab                4.0.12\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.2\n",
      "jupyterlab-widgets        3.0.9\n",
      "kaleido                   0.2.1\n",
      "kiwisolver                1.4.5\n",
      "MarkupSafe                2.1.4\n",
      "matplotlib                3.8.2\n",
      "matplotlib-inline         0.1.6\n",
      "mistune                   3.0.2\n",
      "nbclient                  0.9.0\n",
      "nbconvert                 7.14.2\n",
      "nbformat                  5.9.2\n",
      "nest-asyncio              1.6.0\n",
      "notebook                  7.0.7\n",
      "notebook_shim             0.2.3\n",
      "numpy                     1.26.3\n",
      "overrides                 7.7.0\n",
      "packaging                 23.2\n",
      "pandas                    2.2.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.3\n",
      "pexpect                   4.9.0\n",
      "pillow                    10.2.0\n",
      "pip                       23.3.2\n",
      "platformdirs              4.2.0\n",
      "plotly                    5.18.0\n",
      "prometheus-client         0.19.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.2\n",
      "pycparser                 2.21\n",
      "Pygments                  2.17.2\n",
      "pyparsing                 3.1.1\n",
      "python-dateutil           2.8.2\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2023.4\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "qtconsole                 5.5.1\n",
      "QtPy                      2.4.1\n",
      "referencing               0.33.0\n",
      "requests                  2.31.0\n",
      "retrying                  1.3.4\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.17.1\n",
      "scikit-learn              1.4.0\n",
      "scipy                     1.12.0\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.2\n",
      "setuptools                69.0.2\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.0\n",
      "soupsieve                 2.5\n",
      "stack-data                0.6.3\n",
      "tabulate                  0.9.0\n",
      "tenacity                  8.2.3\n",
      "terminado                 0.18.0\n",
      "threadpoolctl             3.2.0\n",
      "tinycss2                  1.2.1\n",
      "tornado                   6.4\n",
      "traitlets                 5.14.1\n",
      "types-python-dateutil     2.8.19.20240106\n",
      "typing_extensions         4.9.0\n",
      "tzdata                    2023.4\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "Werkzeug                  3.0.1\n",
      "widgetsnbextension        4.0.9\n",
      "zipp                      3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7322b-3a45-4375-bb7e-c6bf47910376",
   "metadata": {},
   "source": [
    "## Exercise 1: The neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e407f-3904-464b-a3cc-91cfddfa65fa",
   "metadata": {},
   "source": [
    "### Task Description\n",
    "\n",
    "The goal of this exercise is to understand the role of a neuron and to implement a neuron.\n",
    "\n",
    "An artificial neuron, the basic unit of the neural network, (also referred to as a perceptron) is a mathematical function. It takes one or more inputs that are multiplied by values called “weights” and added together. This value is then passed to a non-linear function, known as an activation function, to become the neuron’s output.\n",
    "\n",
    "As described in the article, **a neuron takes inputs, does some math with them, and produces one output**.\n",
    "\n",
    "Let us assume there are 2 inputs. Here are the three steps involved in the neuron:\n",
    "\n",
    "1. Each input is multiplied by a weight\n",
    "   - x1 -> x1 \\* w1\n",
    "   - x2 -> x2 \\* w2\n",
    "2. The weighted inputs are added together with a biais b\n",
    "   - (x1 _ w1) + (x2 _ w2) + b\n",
    "3. The sum is passed through an activation function\n",
    "\n",
    "   - y = f((x1 _ w1) + (x2 _ w2) + b)\n",
    "\n",
    "   - The activation function is a function you know from W2DAY2 (Logistic Regression): **the sigmoid**\n",
    "\n",
    "Example:\n",
    "\n",
    "x1 = 2 , x2 = 3 , w1 = 0, w2= 1, b = 4\n",
    "\n",
    "1. Step 1: Multiply by a weight\n",
    "   - x1 -> 2 \\* 0 = 0\n",
    "   - x2 -> 3 \\* 1 = 3\n",
    "2. Step 2: Add weighted inputs and bias\n",
    "   - 0 + 3 + 4 = 7\n",
    "3. Step 3: Activation function\n",
    "   - y = f(7) = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3c261-8b65-48e3-bb31-258757052dc8",
   "metadata": {},
   "source": [
    "**Non-vectorized version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f32792-04be-4f46-9e1a-ea017c5efcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weight1, weight2, bias):\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, input1, input2):\n",
    "        # Multiply inputs with weights and add bias\n",
    "        total = input1 * self.weight1 + input2 * self.weight2 + self.bias\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + math.exp(-total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2732ac-a8dc-43fa-96bc-78a72accf295",
   "metadata": {},
   "source": [
    "#### Audit 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65485d4f-021b-4c12-8433-25c61899480c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990889488055994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = Neuron(0,1,4)\n",
    "neuron.feedforward(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5523e9-ded1-4409-b7d8-3c7323cfdd44",
   "metadata": {},
   "source": [
    "**Vectorized version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e17de1c-1e99-4f3a-9089-f1617d8f247b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990889488055994"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class VectorizedNeuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        # Using dot product for weighted sum\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-total))\n",
    "\n",
    "# Create a vectorized neuron with given weights and bias\n",
    "vectorized_neuron = VectorizedNeuron([0, 1], 4)\n",
    "# Test the neuron with provided inputs as an array\n",
    "vectorized_output = vectorized_neuron.feedforward(np.array([2, 3]))\n",
    "vectorized_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3977e4d-6429-45d7-a2c3-24e80c897720",
   "metadata": {},
   "source": [
    "## Exercise 2: Neural network\n",
    "\n",
    "The goal of this exercise is to understand how to combine three neurons to form a neural network. A neural network is nothing else than neurons connected together. As shown in the figure the neural network is composed of **layers**:\n",
    "\n",
    "- Input layer: it only represents input data. **It doesn't contain neurons**.\n",
    "- Output layer: it represents the last layer. It contains a neuron (in some cases more than 1).\n",
    "- Hidden layer: any layer between the input (first) layer and output (last) layer. Many hidden layers can be stacked. When there are many hidden layers, the neural networks is deep.\n",
    "\n",
    "Notice that the neuron **o1** in the output layer takes as input the output of the neurons **h1** and **h2** in the hidden layer.\n",
    "\n",
    "In exercise 1, you implemented this neuron.\n",
    "\n",
    "Now, we add two more neurons:\n",
    "\n",
    "- h2, the second neuron of the hidden layer\n",
    "- o1, the neuron of the output layer\n",
    "\n",
    "1. Implement the function `feedforward` of the class `OurNeuralNetwork` that takes as input the input data and returns the output y. Return the output for these neurons:\n",
    "\n",
    "   ```\n",
    "   neuron_h1 = Neuron(1,2,-1)\n",
    "   neuron_h2 = Neuron(0.5,1,0)\n",
    "   neuron_o1 = Neuron(2,0,1)\n",
    "   ```\n",
    "\n",
    "   ```\n",
    "   class OurNeuralNetwork:\n",
    "\n",
    "       def __init__(self, neuron_h1, neuron_h2, neuron_o1):\n",
    "           self.h1 = neuron_h1\n",
    "           self.h2 = neuron_h2\n",
    "           self.o1 = neuron_o1\n",
    "\n",
    "       def feedforward(self, x1, x2):\n",
    "       # The inputs for o1 are the outputs from h1 and h2\n",
    "       # TODO\n",
    "           return y\n",
    "\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69bb89e-77e5-4f76-b686-cae4c8058a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525700248057429"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    def __init__(self, neuron_h1, neuron_h2, neuron_o1):\n",
    "        self.h1 = neuron_h1\n",
    "        self.h2 = neuron_h2\n",
    "        self.o1 = neuron_o1\n",
    "\n",
    "    def feedforward(self, x1, x2):\n",
    "        # Outputs from the hidden layer neurons\n",
    "        output_h1 = self.h1.feedforward(x1, x2)\n",
    "        output_h2 = self.h2.feedforward(x1, x2)\n",
    "\n",
    "        # Output from the output layer neuron, using outputs of hidden layer as inputs\n",
    "        y = self.o1.feedforward(output_h1, output_h2)\n",
    "        return y\n",
    "\n",
    "# Initialize neurons with given weights and biases\n",
    "neuron_h1 = Neuron(1, 2, -1)\n",
    "neuron_h2 = Neuron(0.5, 1, 0)\n",
    "neuron_o1 = Neuron(2, 0, 1)\n",
    "\n",
    "# Create an instance of OurNeuralNetwork\n",
    "neural_network = OurNeuralNetwork(neuron_h1, neuron_h2, neuron_o1)\n",
    "\n",
    "# Test the feedforward function with some inputs\n",
    "output = neural_network.feedforward(3, 4)  # Example inputs\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f59160-0b36-4eb1-89f4-2718bec24e63",
   "metadata": {},
   "source": [
    "#### Audit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d72248-ca43-46d4-9e23-502f63e3f39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9524917424084265"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.feedforward(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41846312-c91b-4e03-bdcb-5c6dee185ce5",
   "metadata": {},
   "source": [
    "## Exercise 3: Log loss\n",
    "\n",
    "The objective of this exercise is to implement the Log Loss function, which serves as a **loss function** in classification problems. This function quantifies the difference between predicted and actual categorical outcomes, producing lower values for accurate predictions.\n",
    "\n",
    "Log Loss is a function used in neural networks to help find the best weights for accurate predictions, similar to how we use Mean Squared Error (MSE) to improve predictions in linear regression. While MSE works well for regression (predicting numbers), Log Loss is specifically designed for classification tasks (predicting categories).\n",
    "\n",
    "Log Loss is computed using the formula:\n",
    "\n",
    "`Log loss: - 1/n * Sum[(y_true*log(y_pred) + (1-y_true)\\*log(1-y_pred))]`\n",
    "\n",
    "This equation calculates Log Loss across all predictions in a dataset, penalizing the model more for larger discrepancies between predicted and actual class probabilities.\n",
    "\n",
    "1.  Create a function `log_loss_custom` and compute the loss for the data below:\n",
    "\n",
    "        ```\n",
    "        y_true = np.array([0,1,1,0,1])\n",
    "        y_pred = np.array([0.1,0.8,0.6, 0.5, 0.3])\n",
    "        ```\n",
    "        Check that `log_loss` from `sklearn.metrics` returns the same result\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eacfae5-c17d-47a4-8120-2cec76082c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5472899351247816, 0.5472899351247816)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def log_loss_custom(y_true, y_pred):\n",
    "    # Calculate the log loss for each pair of true and predicted values\n",
    "    n = len(y_true)\n",
    "    loss = -sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / n\n",
    "    return loss\n",
    "\n",
    "# Given data\n",
    "y_true = np.array([0, 1, 1, 0, 1])\n",
    "y_pred = np.array([0.1, 0.8, 0.6, 0.5, 0.3])\n",
    "\n",
    "# Calculate log loss using the custom function\n",
    "custom_log_loss = log_loss_custom(y_true, y_pred)\n",
    "\n",
    "# Calculate log loss using sklearn's function for comparison\n",
    "sklearn_log_loss = log_loss(y_true, y_pred)\n",
    "\n",
    "custom_log_loss, sklearn_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed185b-8114-418b-900f-de5f40a05eec",
   "metadata": {},
   "source": [
    "#### audit 3\n",
    "\n",
    "For question 1, is the output 0.5472899351247816?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80471cf7-bb1a-41a6-9f4c-390da94415e5",
   "metadata": {},
   "source": [
    "## Exercise 4: Forward propagation\n",
    "\n",
    "The goal of this exercise is to compute the log loss on the output of the forward propagation. The data used is the tiny data set below.\n",
    "\n",
    "| name | math | chemistry | exam_success |\n",
    "| :--- | ---: | --------: | -----------: |\n",
    "| Bob  |   12 |        15 |            1 |\n",
    "| Eli  |   10 |         9 |            0 |\n",
    "| Tom  |   18 |        18 |            1 |\n",
    "| Ryan |   13 |        14 |            1 |\n",
    "\n",
    "The goal if the network is to predict the success at the exam given math and chemistry grades. The inputs are `math` and `chemistry` and the target is `exam_success`.\n",
    "\n",
    "1. Compute and return the output of the neural network for each of the students. Here are the weights and biases of the neural network:\n",
    "\n",
    "   ```\n",
    "   neuron_h1 = Neuron(0.05, 0.001, 0)\n",
    "   neuron_h2 = Neuron(0.02, 0.003, 0)\n",
    "   neuron_o1 = Neuron(2,0,0)\n",
    "   ```\n",
    "\n",
    "2. Compute the logloss for the data given the output of the neural network with the 4 students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d5a5dad-8a26-4b95-9bfd-48ba1bbaa6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bob': 0.7855253278357536,\n",
       " 'Eli': 0.7771516558846259,\n",
       " 'Tom': 0.8067873659804015,\n",
       " 'Ryan': 0.7892343955586032}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the neurons with the given weights and biases\n",
    "neuron_h1 = Neuron(0.05, 0.001, 0)\n",
    "neuron_h2 = Neuron(0.02, 0.003, 0)\n",
    "neuron_o1 = Neuron(2, 0, 0)\n",
    "\n",
    "# Creating an instance of the neural network\n",
    "neural_network = OurNeuralNetwork(neuron_h1, neuron_h2, neuron_o1)\n",
    "\n",
    "# Students' data\n",
    "students_data = {\n",
    "    \"Bob\": {\"math\": 12, \"chemistry\": 15, \"exam_success\": 1},\n",
    "    \"Eli\": {\"math\": 10, \"chemistry\": 9, \"exam_success\": 0},\n",
    "    \"Tom\": {\"math\": 18, \"chemistry\": 18, \"exam_success\": 1},\n",
    "    \"Ryan\": {\"math\": 13, \"chemistry\": 14, \"exam_success\": 1}\n",
    "}\n",
    "\n",
    "# Computing the output for each student\n",
    "outputs = {name: neural_network.feedforward(data[\"math\"], data[\"chemistry\"]) \n",
    "           for name, data in students_data.items()}\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e756940-ad48-4e71-87d3-8236453f99bf",
   "metadata": {},
   "source": [
    "#### Audit 4.1\n",
    "\n",
    "```\n",
    "Bob: 0.7855253278357536\n",
    "Eli: 0.7771516558846259\n",
    "Tom: 0.8067873659804015\n",
    "Ryan: 0.7892343955586032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c318be7c-cc1c-42a0-b3ad-13ba1b9f6567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5485133607757963"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the actual labels (exam_success) and predicted probabilities\n",
    "y_true = np.array([data[\"exam_success\"] for data in students_data.values()])\n",
    "y_pred = np.array(list(outputs.values()))\n",
    "\n",
    "# Calculate log loss for the dataset\n",
    "log_loss_dataset = log_loss_custom(y_true, y_pred)\n",
    "log_loss_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a277d-321f-4072-9427-8aab70068edb",
   "metadata": {},
   "source": [
    "#### Audit 4.2\n",
    "\n",
    "is the logloss for the 4 students 0.5485133607757963?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfdf1a4-e351-4474-a082-2bd5eaa4ab6b",
   "metadata": {},
   "source": [
    "## Exercise 5: Regression\n",
    "\n",
    "The goal of this exercise is to learn to adapt the output layer to regression.\n",
    "As a reminder, one of reasons for which the sigmoid is used in classification is because it contracts the output between 0 and 1 which is the expected output range for a probability (W2D2: Logistic regression). However, the output of the regression is not a probability.\n",
    "\n",
    "In order to perform a regression using a neural network, the activation function of the neuron on the output layer has to be modified to **identity function**. In mathematics, the identity function is: **f(x) = x**. In other words it means that it returns the input as so. The three steps become:\n",
    "\n",
    "1. Each input is multiplied by a weight\n",
    "   - x1 -> x1 \\* w1\n",
    "   - x2 -> x2 \\* w2\n",
    "2. The weighted inputs are added together with a biais b\n",
    "   - (x1 _ w1) + (x2 _ w2) + b\n",
    "3. The sum is passed through an activation function\n",
    "   - y = f((x1 _ w1) + (x2 _ w2) + b)\n",
    "   - The activation function is **the identity**\n",
    "   - y = (x1 _ w1) + (x2 _ w2) + b\n",
    "\n",
    "All other neurons' activation function **doesn't change**.\n",
    "\n",
    "1. Adapt the neuron class implemented in exercise 1. It now takes as a parameter `regression` which is boolean. When its value is `True`, `feedforward` should use the identity function as activation function instead of the sigmoid function.\n",
    "\n",
    "   ```\n",
    "   class Neuron:\n",
    "   def __init__(self, weight1, weight2, bias, regression):\n",
    "       self.weights_1 = weight1\n",
    "       self.weights_2 = weight2\n",
    "       self.bias = bias\n",
    "       #TODO\n",
    "\n",
    "   def feedforward(self, x1, x2):\n",
    "       #TODO\n",
    "       return y\n",
    "\n",
    "   ```\n",
    "\n",
    "   - Compute the output for:\n",
    "\n",
    "     ```\n",
    "     neuron = Neuron(0,1,4, True)\n",
    "     neuron.feedforward(2,3)\n",
    "     ```\n",
    "\n",
    "2. Now, the goal of the network is to predict the physics' grade at the exam given math and chemistry grades. The inputs are `math` and `chemistry` and the target is `physics`.\n",
    "\n",
    "| name | math | chemistry | physics |\n",
    "| :--- | ---: | --------: | ------: |\n",
    "| Bob  |   12 |        15 |      16 |\n",
    "| Eli  |   10 |         9 |      10 |\n",
    "| Tom  |   18 |        18 |      19 |\n",
    "| Ryan |   13 |        14 |      16 |\n",
    "\n",
    "Compute and return the output of the neural network for each of the students. Here are the weights and biases of the neural network:\n",
    "\n",
    "```\n",
    "    #replace regression by the right value\n",
    "    neuron_h1 = Neuron(0.05, 0.001, 0, regression)\n",
    "    neuron_h2 = Neuron(0.002, 0.003, 0, regression)\n",
    "    neuron_o1 = Neuron(2,7,10, regression)\n",
    "```\n",
    "\n",
    "3. Compute the MSE for the 4 students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18788096-e931-40c7-8f57-2a8927cbdfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs: {'Bob': 14.918863163724454, 'Eli': 14.83137890625537, 'Tom': 15.086662606964074, 'Ryan': 14.939270885974128}\n",
      "MSE: 10.237608699909138\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weight1, weight2, bias, regression=False):\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.bias = bias\n",
    "        self.regression = regression\n",
    "\n",
    "    def feedforward(self, x1, x2):\n",
    "        total = x1 * self.weight1 + x2 * self.weight2 + self.bias\n",
    "        if self.regression:\n",
    "            # For regression, use the identity function\n",
    "            return total\n",
    "        else:\n",
    "            # For classification, use the sigmoid function\n",
    "            return 1 / (1 + np.exp(-total))\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self, neuron_h1, neuron_h2, neuron_o1):\n",
    "        self.h1 = neuron_h1\n",
    "        self.h2 = neuron_h2\n",
    "        self.o1 = neuron_o1\n",
    "\n",
    "    def feedforward(self, x1, x2):\n",
    "        output_h1 = self.h1.feedforward(x1, x2)\n",
    "        output_h2 = self.h2.feedforward(x1, x2)\n",
    "        return self.o1.feedforward(output_h1, output_h2)\n",
    "\n",
    "# Initialize neurons, only output neuron is for regression\n",
    "neuron_h1 = Neuron(0.05, 0.001, 0, False)\n",
    "neuron_h2 = Neuron(0.002, 0.003, 0, False)\n",
    "neuron_o1 = Neuron(2, 7, 10, True)\n",
    "\n",
    "# Neural network instance\n",
    "neural_network = OurNeuralNetwork(neuron_h1, neuron_h2, neuron_o1)\n",
    "\n",
    "# Students' data\n",
    "students_data = {\n",
    "    \"Bob\": {\"math\": 12, \"chemistry\": 15, \"physics\": 16},\n",
    "    \"Eli\": {\"math\": 10, \"chemistry\": 9, \"physics\": 10},\n",
    "    \"Tom\": {\"math\": 18, \"chemistry\": 18, \"physics\": 19},\n",
    "    \"Ryan\": {\"math\": 13, \"chemistry\": 14, \"physics\": 16}\n",
    "}\n",
    "\n",
    "# Compute outputs and MSE\n",
    "outputs = {name: neural_network.feedforward(data[\"math\"], data[\"chemistry\"]) \n",
    "           for name, data in students_data.items()}\n",
    "\n",
    "y_true = np.array([data[\"physics\"] for data in students_data.values()])\n",
    "y_pred = np.array(list(outputs.values()))\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "\n",
    "print(\"Outputs:\", outputs)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb8f1c16-d587-457c-9a79-f8e5e5acda2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output: 7\n",
      "Outputs: {'Bob': 14.918863163724454, 'Eli': 14.83137890625537, 'Tom': 15.086662606964074, 'Ryan': 14.939270885974128}\n",
      "MSE: 10.237608699909138\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weight1, weight2, bias, regression=False):\n",
    "        self.weight1 = weight1\n",
    "        self.weight2 = weight2\n",
    "        self.bias = bias\n",
    "        self.regression = regression\n",
    "\n",
    "    def feedforward(self, x1, x2):\n",
    "        total = x1 * self.weight1 + x2 * self.weight2 + self.bias\n",
    "        if self.regression:\n",
    "            # For regression, use the identity function\n",
    "            return total\n",
    "        else:\n",
    "            # For classification, use the sigmoid function\n",
    "            return 1 / (1 + np.exp(-total))\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self, neuron_h1, neuron_h2, neuron_o1):\n",
    "        self.h1 = neuron_h1\n",
    "        self.h2 = neuron_h2\n",
    "        self.o1 = neuron_o1\n",
    "\n",
    "    def feedforward(self, x1, x2):\n",
    "        output_h1 = self.h1.feedforward(x1, x2)\n",
    "        output_h2 = self.h2.feedforward(x1, x2)\n",
    "        return self.o1.feedforward(output_h1, output_h2)\n",
    "\n",
    "# Test case for Neuron class\n",
    "neuron_test = Neuron(0, 1, 4, True)\n",
    "test_output = neuron_test.feedforward(2, 3)\n",
    "print(\"Test Output:\", test_output)  # Expected to be 7\n",
    "\n",
    "# Initialize neurons, only output neuron is for regression\n",
    "neuron_h1 = Neuron(0.05, 0.001, 0, False)\n",
    "neuron_h2 = Neuron(0.002, 0.003, 0, False)\n",
    "neuron_o1 = Neuron(2, 7, 10, True)\n",
    "\n",
    "# Neural network instance\n",
    "neural_network = OurNeuralNetwork(neuron_h1, neuron_h2, neuron_o1)\n",
    "\n",
    "# Students' data\n",
    "students_data = {\n",
    "    \"Bob\": {\"math\": 12, \"chemistry\": 15, \"physics\": 16},\n",
    "    \"Eli\": {\"math\": 10, \"chemistry\": 9, \"physics\": 10},\n",
    "    \"Tom\": {\"math\": 18, \"chemistry\": 18, \"physics\": 19},\n",
    "    \"Ryan\": {\"math\": 13, \"chemistry\": 14, \"physics\": 16}\n",
    "}\n",
    "\n",
    "# Compute outputs and MSE\n",
    "outputs = {name: neural_network.feedforward(data[\"math\"], data[\"chemistry\"]) \n",
    "           for name, data in students_data.items()}\n",
    "\n",
    "y_true = np.array([data[\"physics\"] for data in students_data.values()])\n",
    "y_pred = np.array(list(outputs.values()))\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "\n",
    "print(\"Outputs:\", outputs)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d816d-d2aa-48fd-bed3-7abeed023ca4",
   "metadata": {},
   "source": [
    "#### Audit 5\n",
    "\n",
    "For question 1, is the output 7?\n",
    "For question 2, are the outputs the following?\n",
    "```\n",
    "Bob: 14.918863163724454\n",
    "Eli: 14.83137890625537\n",
    "Tom: 15.086662606964074\n",
    "Ryan: 14.939270885974128\n",
    "```\n",
    "For question 3, is the MSE 10.237608699909138?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
